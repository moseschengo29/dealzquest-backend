import random
import uuid
import requests
from bs4 import BeautifulSoup

def generate_dummy_product():
    """Generate a dummy product for demo purposes"""
    product_names = [
        "Samsung Galaxy S21", "iPhone 13 Pro", "MacBook Air M1", "LG OLED TV", 
        "Sony PlayStation 5", "Nintendo Switch", "Canon EOS R5", "Dell XPS 13",
        "Bose QuietComfort Earbuds", "JBL Flip 5", "Nike Air Max", "Adidas Ultraboost",
        "DJI Mavic Air 2", "Instant Pot Duo", "Dyson V11", "Kindle Paperwhite",
        "Fitbit Charge 5", "Apple Watch Series 7", "Samsung 970 EVO SSD", "Logitech MX Master 3"
    ]
    
    stores = ["Jumia Kenya", "Kilimall", "Masoko", "SkyGarden", "Jiji Kenya"]
    
    product_id = str(uuid.uuid4())[:8]
    name = random.choice(product_names)
    price = random.randint(1000, 200000)  # Price in KSh
    image = f"https://picsum.photos/seed/{product_id}/400/400"
    source = random.choice(stores)
    url = f"https://example.com/product/{product_id}"
    rating = round(random.uniform(3.0, 5.0), 1)
    
    return {
        "id": product_id,
        "name": name,
        "price": price,
        "image": image,
        "source": source,
        "url": url,
        "rating": rating
    }

# def scrape_products(query):
#     """
#     Simulate scraping products from multiple Kenyan e-commerce sites
#     In a real implementation, this would use BeautifulSoup to scrape actual sites
#     """
#     # For demo purposes, generate random products
#     num_products = random.randint(8, 20)
#     products = [generate_dummy_product() for _ in range(num_products)]
    
#     # If there's a query, filter products to make it seem like a real search
#     if query:
#         filtered_products = []
#         for product in products:
#             # Make 30% of products match the query exactly
#             if random.random() < 0.3:
#                 product['name'] = f"{query} {product['name'].split(' ')[-1]}"
#                 filtered_products.append(product)
#             # Make another 40% of products contain the query somewhere
#             elif random.random() < 0.7:
#                 if random.random() < 0.5:
#                     product['name'] = f"{product['name']} {query}"
#                 else:
#                     parts = product['name'].split(' ')
#                     if len(parts) > 1:
#                         product['name'] = f"{parts[0]} {query} {' '.join(parts[1:])}"
#                     else:
#                         product['name'] = f"{parts[0]} {query}"
#                 filtered_products.append(product)
        
#         # If we have too few results, add some of the original products
#         if len(filtered_products) < 5:
#             filtered_products.extend(products[:5-len(filtered_products)])
        
#         return filtered_products
    
#     return products

def scrape_products(query):
    results = []

    results.extend(scrape_jumia(query))
    results.extend(scrape_kilimall(query))
    results.extend(scrape_masoko(query))
    results.extend(scrape_jiji(query))
    results.extend(scrape_amazon(query))

    random.shuffle(results)
    return results[:20]

def scrape_jumia(query):
    try:
        url = f"https://www.jumia.co.ke/catalog/?q={query}"
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        response = requests.get(url, headers=headers)

        if response.status_code != 200:
            return []

        soup = BeautifulSoup(response.content, 'html.parser')
        products = []

        product_cards = soup.select('article.prd')

        for card in product_cards:
            try:
                name = card.select_one('h3.name').text.strip()
                price_text = card.select_one('.prc').text.strip().replace('KSh', '').replace(',', '')
                price = int(float(price_text))
                image = card.select_one('img').get('data-src') or card.select_one('img').get('src')
                url_suffix = card.select_one('a.core').get('href')
                url = f"https://www.jumia.co.ke{url_suffix}"
                product_id = str(uuid.uuid4())[:8]

                products.append({
                    "id": product_id,
                    "name": name,
                    "price": price,
                    "image": image,
                    "source": "Jumia Kenya",
                    "url": url,
                    "rating": round(random.uniform(3.0, 5.0), 1)
                })
            except Exception as e:
                print(f"Error parsing a Jumia product: {e}")
                continue

        return products
    except Exception as e:
        print(f"Error scraping Jumia: {e}")
        return []

def scrape_kilimall(query):
    try:
        url = f"https://www.kilimall.co.ke/search?keyword={query}"
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko)'
        }
        response = requests.get(url, headers=headers)
        if response.status_code != 200:
            return []

        soup = BeautifulSoup(response.content, 'html.parser')
        products = []

        product_cards = soup.select('.product-box')

        for card in product_cards:
            try:
                name = card.select_one('.title').text.strip()
                price_text = card.select_one('.price-now').text.strip().replace('KSh', '').replace(',', '')
                price = int(float(price_text))
                image = card.select_one('img').get('data-original')
                url = card.select_one('a').get('href')
                if not url.startswith('http'):
                    url = f"https://www.kilimall.co.ke{url}"

                products.append({
                    "id": str(uuid.uuid4())[:8],
                    "name": name,
                    "price": price,
                    "image": image,
                    "source": "Kilimall",
                    "url": url,
                    "rating": round(random.uniform(3.0, 5.0), 1)
                })
            except Exception as e:
                print(f"Error parsing Kilimall product: {e}")
                continue

        return products
    except Exception as e:
        print(f"Error scraping Kilimall: {e}")
        return []
    
def scrape_masoko(query):
    # Implementation for Masoko
    pass


def scrape_jiji(query):
    try:
        url = f"https://jiji.co.ke/search?query={query}"
        headers = {
            'User-Agent': 'Mozilla/5.0'
        }
        response = requests.get(url, headers=headers)
        if response.status_code != 200:
            return []

        soup = BeautifulSoup(response.content, 'html.parser')
        products = []

        product_cards = soup.select('.b-list-advert__item')

        for card in product_cards:
            try:
                name = card.select_one('.b-advert-title-inner').text.strip()
                price_text = card.select_one('.b-advert-price-inner').text.strip().replace('KSh', '').replace(',', '')
                price = int(float(price_text))
                image = card.select_one('img').get('data-src') or card.select_one('img').get('src')
                url = 'https://jiji.co.ke' + card.select_one('a').get('href')

                products.append({
                    "id": str(uuid.uuid4())[:8],
                    "name": name,
                    "price": price,
                    "image": image,
                    "source": "Jiji Kenya",
                    "url": url,
                    "rating": round(random.uniform(3.0, 5.0), 1)
                })
            except Exception as e:
                print(f"Error parsing Jiji product: {e}")
                continue

        return products
    except Exception as e:
        print(f"Error scraping Jiji: {e}")
        return []


def scrape_masoko(query):
    try:
        url = f"https://www.masoko.com/catalogsearch/result/?q={query}"
        headers = {
            'User-Agent': 'Mozilla/5.0'
        }
        response = requests.get(url, headers=headers)
        if response.status_code != 200:
            return []

        soup = BeautifulSoup(response.content, 'html.parser')
        products = []

        product_cards = soup.select('.product-item')

        for card in product_cards:
            try:
                name = card.select_one('.product-item-name').text.strip()
                price_text = card.select_one('.price').text.strip().replace('KSh', '').replace(',', '')
                price = int(float(price_text))
                image = card.select_one('img').get('src')
                url = card.select_one('a').get('href')

                products.append({
                    "id": str(uuid.uuid4())[:8],
                    "name": name,
                    "price": price,
                    "image": image,
                    "source": "Masoko",
                    "url": url,
                    "rating": round(random.uniform(3.0, 5.0), 1)
                })
            except Exception as e:
                print(f"Error parsing Masoko product: {e}")
                continue

        return products
    except Exception as e:
        print(f"Error scraping Masoko: {e}")
        return []

import re

def scrape_amazon(query):
    try:
        base_url = "https://www.amazon.com"
        search_url = f"{base_url}/s?k={query.replace(' ', '+')}"
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
            "Accept-Language": "en-US,en;q=0.9"
        }
        response = requests.get(search_url, headers=headers)

        if response.status_code != 200:
            print(f"Amazon returned status code {response.status_code}")
            return []

        soup = BeautifulSoup(response.content, "html.parser")
        product_cards = soup.select('div.s-main-slot div[data-component-type="s-search-result"]')
        products = []

        for card in product_cards:
            try:
                title_tag = card.select_one('h2 a span')
                if not title_tag:
                    continue

                name = title_tag.text.strip()
                url_suffix = card.select_one('h2 a').get('href')
                url = base_url + url_suffix

                price_whole = card.select_one('.a-price .a-price-whole')
                price_frac = card.select_one('.a-price .a-price-fraction')
                if price_whole and price_frac:
                    price_str = f"{price_whole.text.strip()}.{price_frac.text.strip()}"
                    price = int(float(price_str) * 150)  # Approx conversion to KSh
                else:
                    continue  # Skip if price not found

                image = card.select_one('img').get('src')

                products.append({
                    "id": str(uuid.uuid4())[:8],
                    "name": name,
                    "price": price,
                    "image": image,
                    "source": "Amazon",
                    "url": url,
                    "rating": round(random.uniform(3.0, 5.0), 1)
                })
            except Exception as e:
                print(f"Error parsing Amazon product: {e}")
                continue

        return products
    except Exception as e:
        print(f"Error scraping Amazon: {e}")
        return []
